{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"9c3fe67966fadd35898ea10876c2d25074ac0ea19f3e5bbc4643b728a1d5eddb"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:30:05.307596Z","iopub.execute_input":"2022-10-10T07:30:05.308016Z","iopub.status.idle":"2022-10-10T07:30:05.314597Z","shell.execute_reply.started":"2022-10-10T07:30:05.307974Z","shell.execute_reply":"2022-10-10T07:30:05.313142Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler\n\nimport transformers\nfrom transformers import XLMRobertaModel, XLMRobertaTokenizer, XLMRobertaConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n\nimport re","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:30:05.617481Z","iopub.execute_input":"2022-10-10T07:30:05.617910Z","iopub.status.idle":"2022-10-10T07:30:13.426971Z","shell.execute_reply.started":"2022-10-10T07:30:05.617872Z","shell.execute_reply":"2022-10-10T07:30:13.425891Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 224\npath = \"../input/modelka\"\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:30:13.430035Z","iopub.execute_input":"2022-10-10T07:30:13.430728Z","iopub.status.idle":"2022-10-10T07:30:13.497265Z","shell.execute_reply.started":"2022-10-10T07:30:13.430688Z","shell.execute_reply":"2022-10-10T07:30:13.496328Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, df):\n        self.texts = df['text'].values\n        self.labels = df['lang'].values\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n\n    def get_tokens(self, text):\n        encoded = self.tokenizer.encode_plus(text, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True)\n        return encoded['input_ids'], encoded['attention_mask']\n\n    def __len__(self):\n        return self.labels.shape[0]\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = torch.tensor(int(self.labels[idx])).long()\n        target = onehot(2, label)\n        tokens, attention_mask = self.get_tokens(text)\n        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n\n        return target, tokens, attention_mask","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:30:13.499045Z","iopub.execute_input":"2022-10-10T07:30:13.499506Z","iopub.status.idle":"2022-10-10T07:30:13.510324Z","shell.execute_reply.started":"2022-10-10T07:30:13.499460Z","shell.execute_reply":"2022-10-10T07:30:13.509377Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CustomRoberta(nn.Module):\n    def __init__(self):\n        super(CustomRoberta, self).__init__()\n        self.num_labels = 2\n        self.roberta = transformers.XLMRobertaModel.from_pretrained(path)\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(\n            in_features=self.roberta.pooler.dense.out_features*2,\n            out_features=2,\n        )\n        \n\n    def forward(self,\n                input_ids=None,\n                attention_mask=None,\n                position_ids=None,\n                head_mask=None,\n                inputs_embeds=None):\n\n        o1, o2 = self.roberta(input_ids,\n                               attention_mask=attention_mask,\n                               position_ids=position_ids,\n                               head_mask=head_mask,\n                               inputs_embeds=inputs_embeds)\n        apool = torch.mean(o1, 1)\n        mpool, _ = torch.max(o1, 1)\n        x = torch.cat((apool, mpool), 1)\n        x = self.dropout(x)\n        return self.linear(x)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:30:13.513802Z","iopub.execute_input":"2022-10-10T07:30:13.514169Z","iopub.status.idle":"2022-10-10T07:30:13.524421Z","shell.execute_reply.started":"2022-10-10T07:30:13.514124Z","shell.execute_reply":"2022-10-10T07:30:13.523483Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = CustomRoberta()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:30:13.525751Z","iopub.execute_input":"2022-10-10T07:30:13.526265Z","iopub.status.idle":"2022-10-10T07:30:28.172861Z","shell.execute_reply.started":"2022-10-10T07:30:13.526230Z","shell.execute_reply":"2022-10-10T07:30:28.171948Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/modelka were not used when initializing XLMRobertaModel: ['linear.bias', 'linear.weight']\n- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"../input/modelka/pytorch_model.bin\", map_location=device))","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:30:28.174249Z","iopub.execute_input":"2022-10-10T07:30:28.174949Z","iopub.status.idle":"2022-10-10T07:30:34.292481Z","shell.execute_reply.started":"2022-10-10T07:30:28.174907Z","shell.execute_reply":"2022-10-10T07:30:34.291585Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"def eval(testloader, model):\n    model.eval()\n    model = model.to(device)\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n    counter = 0\n    for step, (targets, inputs, attention_masks) in enumerate(testloader):\n        with torch.no_grad():\n            inputs = inputs.to(device)\n            attention_masks = attention_masks.to(device)\n            targets = targets.to(device)\n            output = model(inputs, attention_masks)\n            correct_predictions += (output.argmax(1) == targets.argmax(1)).type(torch.float).sum().item()\n            total_predictions += len(targets)\n    print(f\"Accuracy: {correct_predictions / total_predictions}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:32:20.545702Z","iopub.execute_input":"2022-10-10T07:32:20.546205Z","iopub.status.idle":"2022-10-10T07:32:20.555804Z","shell.execute_reply.started":"2022-10-10T07:32:20.546160Z","shell.execute_reply":"2022-10-10T07:32:20.553904Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/datalang/test_data.csv\")\ndata['lang'] = np.where(data['lang'] == 'kz', 0, 1)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:32:31.500733Z","iopub.execute_input":"2022-10-10T07:32:31.501126Z","iopub.status.idle":"2022-10-10T07:32:31.529283Z","shell.execute_reply.started":"2022-10-10T07:32:31.501073Z","shell.execute_reply":"2022-10-10T07:32:31.528212Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"test_set = DatasetRetriever(data)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:32:31.626124Z","iopub.execute_input":"2022-10-10T07:32:31.626436Z","iopub.status.idle":"2022-10-10T07:32:33.854348Z","shell.execute_reply.started":"2022-10-10T07:32:31.626408Z","shell.execute_reply":"2022-10-10T07:32:33.853150Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"eval(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T07:32:34.204377Z","iopub.execute_input":"2022-10-10T07:32:34.205128Z","iopub.status.idle":"2022-10-10T07:33:26.874461Z","shell.execute_reply.started":"2022-10-10T07:32:34.205074Z","shell.execute_reply":"2022-10-10T07:33:26.873423Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.9994509265614276\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}