{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-09T11:59:18.253835Z","iopub.execute_input":"2022-10-09T11:59:18.254132Z","iopub.status.idle":"2022-10-09T12:00:37.018677Z","shell.execute_reply.started":"2022-10-09T11:59:18.254103Z","shell.execute_reply":"2022-10-09T12:00:37.017306Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  6034  100  6034    0     0  33513      0 --:--:-- --:--:-- --:--:-- 33709\nUpdating... This may take around 2 minutes.\nUpdating TPU runtime to pytorch-nightly ...\nFound existing installation: torch 1.7.1+cpu\nUninstalling torch-1.7.1+cpu:\n  Successfully uninstalled torch-1.7.1+cpu\nFound existing installation: torchvision 0.8.2+cpu\nUninstalling torchvision-0.8.2+cpu:\n  Successfully uninstalled torchvision-0.8.2+cpu\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nCopying gs://tpu-pytorch/wheels/colab/torch-nightly-cp37-cp37m-linux_x86_64.whl...\n| [1 files][112.5 MiB/112.5 MiB]                                                \nOperation completed over 1 objects/112.5 MiB.                                    \nCopying gs://tpu-pytorch/wheels/colab/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\n- [1 files][143.6 MiB/143.6 MiB]                                                \nOperation completed over 1 objects/143.6 MiB.                                    \nDone updating TPU runtime\nCopying gs://tpu-pytorch/wheels/colab/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][  5.8 MiB/  5.8 MiB]                                                \nOperation completed over 1 objects/5.8 MiB.                                      \nProcessing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (3.7.4.3)\nInstalling collected packages: torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.2.7 requires torchvision<0.9,>=0.8, which is not installed.\neasyocr 1.4.1 requires torchvision>=0.5, which is not installed.\nallennlp 2.7.0 requires torchvision<0.11.0,>=0.8.1, which is not installed.\ntorchtext 0.8.1 requires torch==1.7.1, but you have torch 1.14.0a0+git7134b9b which is incompatible.\ntorchaudio 0.7.2 requires torch==1.7.1, but you have torch 1.14.0a0+git7134b9b which is incompatible.\nfastai 2.2.7 requires torch<1.8,>=1.7.0, but you have torch 1.14.0a0+git7134b9b which is incompatible.\nallennlp 2.7.0 requires torch<1.10.0,>=1.6.0, but you have torch 1.14.0a0+git7134b9b which is incompatible.\u001b[0m\nSuccessfully installed torch-1.14.0a0+git7134b9b\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nProcessing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: cloud-tpu-client>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from torch-xla==nightly) (0.10)\nCollecting absl-py>=1.0.0\n  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n\u001b[K     |████████████████████████████████| 123 kB 2.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from cloud-tpu-client>=0.10.0->torch-xla==nightly) (4.1.3)\nRequirement already satisfied: google-api-python-client==1.8.0 in /opt/conda/lib/python3.7/site-packages (from cloud-tpu-client>=0.10.0->torch-xla==nightly) (1.8.0)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (0.1.0)\nRequirement already satisfied: google-api-core<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (1.31.1)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (3.0.1)\nRequirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (1.34.0)\nRequirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (0.19.1)\nRequirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (1.15.0)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (2021.1)\nRequirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (21.0)\nRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (57.4.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (2.25.1)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (1.53.0)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (3.18.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (4.2.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (4.7.2)\nRequirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (2.4.7)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (0.4.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (1.26.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (2021.5.30)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==nightly) (4.0.0)\nInstalling collected packages: absl-py, torch-xla\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 0.12.0\n    Uninstalling absl-py-0.12.0:\n      Successfully uninstalled absl-py-0.12.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.4.1 requires absl-py~=0.10, but you have absl-py 1.2.0 which is incompatible.\ntensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 1.2.0 which is incompatible.\u001b[0m\nSuccessfully installed absl-py-1.2.0 torch-xla-1.13+8332ac9\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nProcessing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (3.7.4.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (8.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (2.25.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.14.0a0+git7134b9b)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.19.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==nightly) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==nightly) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==nightly) (1.26.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==nightly) (2021.5.30)\nInstalling collected packages: torchvision\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.2.7 requires torch<1.8,>=1.7.0, but you have torch 1.14.0a0+git7134b9b which is incompatible.\nfastai 2.2.7 requires torchvision<0.9,>=0.8, but you have torchvision 0.15.0a0+6e203b4 which is incompatible.\nallennlp 2.7.0 requires torch<1.10.0,>=1.6.0, but you have torch 1.14.0a0+git7134b9b which is incompatible.\nallennlp 2.7.0 requires torchvision<0.11.0,>=0.8.1, but you have torchvision 0.15.0a0+6e203b4 which is incompatible.\u001b[0m\nSuccessfully installed torchvision-0.15.0a0+6e203b4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  libomp5 libopenblas-dev\n0 upgraded, 2 newly installed, 0 to remove and 18 not upgraded.\nNeed to get 4094 kB of archives.\nAfter this operation, 54.2 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\nFetched 4094 kB in 1s (6887 kB/s)\ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libopenblas-dev:amd64.\n(Reading database ... 106894 files and directories currently installed.)\nPreparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libomp5:amd64.\nPreparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\nUnpacking libomp5:amd64 (5.0.1-1) ...\nSetting up libomp5:amd64 (5.0.1-1) ...\nSetting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\nProcessing triggers for libc-bin (2.27-3ubuntu1.4) ...\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nimport gc\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\nfrom joblib import Parallel, delayed\n\nimport torch_xla.utils.serialization as xser\n\nimport time\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.autograd import Variable\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nimport sklearn\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig, get_cosine_schedule_with_warmup\nimport sys\nfrom sklearn import metrics, model_selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.notebook import tqdm\n\nfrom random import shuffle\nimport random\n\nimport re\n\nimport warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:00:37.022995Z","iopub.execute_input":"2022-10-09T12:00:37.024048Z","iopub.status.idle":"2022-10-09T12:00:45.812423Z","shell.execute_reply.started":"2022-10-09T12:00:37.023984Z","shell.execute_reply":"2022-10-09T12:00:45.811309Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2022-10-09 12:00:40.400847: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/datalang/data1.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:00:45.813948Z","iopub.execute_input":"2022-10-09T12:00:45.814347Z","iopub.status.idle":"2022-10-09T12:00:47.171251Z","shell.execute_reply.started":"2022-10-09T12:00:45.814304Z","shell.execute_reply":"2022-10-09T12:00:47.170118Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:00:47.173448Z","iopub.execute_input":"2022-10-09T12:00:47.173692Z","iopub.status.idle":"2022-10-09T12:00:47.204940Z","shell.execute_reply.started":"2022-10-09T12:00:47.173664Z","shell.execute_reply":"2022-10-09T12:00:47.204034Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        Unnamed: 0                                               text  lang\n0                0  екі мың бес-екі мың алтыншы жылдары қазақстан ...     0\n1                1  туристерге қолайлы жағдай жасау біздің басты м...     0\n2                2  алдағы уақытта ауыл тұрғындарының тұрмыстық жа...     0\n3                3                               бақылауында ұстайды.     0\n4                4                           шаршамаңдар, көп рақмет!     0\n...            ...                                                ...   ...\n256900      256900  А благодарность? Для чего? Это тебя мы должны ...     1\n256901      256901                                 Ты уходишь от нас?     1\n256902      256902                     О нет. Мы всегда будем вместе.     1\n256903      256903  Совет кардиналов! Я так нервничаю! Что, если я...     1\n256904      256904                Прямо в ад. Шучу. Где этот автобус?     1\n\n[256905 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>екі мың бес-екі мың алтыншы жылдары қазақстан ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>туристерге қолайлы жағдай жасау біздің басты м...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>алдағы уақытта ауыл тұрғындарының тұрмыстық жа...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>бақылауында ұстайды.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>шаршамаңдар, көп рақмет!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>256900</th>\n      <td>256900</td>\n      <td>А благодарность? Для чего? Это тебя мы должны ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>256901</th>\n      <td>256901</td>\n      <td>Ты уходишь от нас?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>256902</th>\n      <td>256902</td>\n      <td>О нет. Мы всегда будем вместе.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>256903</th>\n      <td>256903</td>\n      <td>Совет кардиналов! Я так нервничаю! Что, если я...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>256904</th>\n      <td>256904</td>\n      <td>Прямо в ад. Шучу. Где этот автобус?</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>256905 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()\n!free -h","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:00:47.206244Z","iopub.execute_input":"2022-10-09T12:00:47.206480Z","iopub.status.idle":"2022-10-09T12:00:48.508908Z","shell.execute_reply.started":"2022-10-09T12:00:47.206454Z","shell.execute_reply":"2022-10-09T12:00:48.507715Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"              total        used        free      shared  buff/cache   available\nMem:            17G        892M         10G        1.0M        6.5G         16G\nSwap:            0B          0B          0B\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LEN = 224","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:00:48.510588Z","iopub.execute_input":"2022-10-09T12:00:48.510900Z","iopub.status.idle":"2022-10-09T12:00:48.516091Z","shell.execute_reply.started":"2022-10-09T12:00:48.510863Z","shell.execute_reply":"2022-10-09T12:00:48.514778Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, df):\n        self.texts = df['text'].values\n        self.labels = df['lang'].values\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n\n    def get_tokens(self, text):\n        encoded = self.tokenizer.encode_plus(text, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True)\n        return encoded['input_ids'], encoded['attention_mask']\n\n    def __len__(self):\n        return self.labels.shape[0]\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = torch.tensor(int(self.labels[idx])).long()\n        target = onehot(2, label)\n        tokens, attention_mask = self.get_tokens(text)\n        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n\n        return target, tokens, attention_mask","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:00:48.517993Z","iopub.execute_input":"2022-10-09T12:00:48.518261Z","iopub.status.idle":"2022-10-09T12:00:48.529741Z","shell.execute_reply.started":"2022-10-09T12:00:48.518231Z","shell.execute_reply":"2022-10-09T12:00:48.528772Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CustomRoberta(nn.Module):\n    def __init__(self):\n        super(CustomRoberta, self).__init__()\n        self.num_labels = 2\n        self.roberta = transformers.XLMRobertaModel.from_pretrained(\"xlm-roberta-base\", output_hidden_states=False, num_labels=1, return_dict=False)\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(\n            in_features=self.roberta.pooler.dense.out_features*2,\n            out_features=2,\n        )\n        \n\n    def forward(self,\n                input_ids=None,\n                attention_mask=None,\n                position_ids=None,\n                head_mask=None,\n                inputs_embeds=None):\n\n        o1, o2 = self.roberta(input_ids,\n                               attention_mask=attention_mask,\n                               position_ids=position_ids,\n                               head_mask=head_mask,\n                               inputs_embeds=inputs_embeds)\n        apool = torch.mean(o1, 1)\n        mpool, _ = torch.max(o1, 1)\n        x = torch.cat((apool, mpool), 1)\n        x = self.dropout(x)\n        return self.linear(x)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:00:48.549150Z","iopub.execute_input":"2022-10-09T12:00:48.549952Z","iopub.status.idle":"2022-10-09T12:00:48.563406Z","shell.execute_reply.started":"2022-10-09T12:00:48.549905Z","shell.execute_reply":"2022-10-09T12:00:48.562103Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"mx = CustomRoberta()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:00:48.566479Z","iopub.execute_input":"2022-10-09T12:00:48.566810Z","iopub.status.idle":"2022-10-09T12:01:18.144743Z","shell.execute_reply.started":"2022-10-09T12:00:48.566773Z","shell.execute_reply":"2022-10-09T12:01:18.143912Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"078908a54a2544f285f5fb5ee285aa92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8f180cae9e4ddea6cba7186ed7b0db"}},"metadata":{}}]},{"cell_type":"code","source":"os.makedirs(\"./inference\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:18.147429Z","iopub.execute_input":"2022-10-09T12:01:18.147910Z","iopub.status.idle":"2022-10-09T12:01:18.157829Z","shell.execute_reply.started":"2022-10-09T12:01:18.147840Z","shell.execute_reply":"2022-10-09T12:01:18.157091Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"mx.roberta.save_pretrained(\"./inference\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:18.160934Z","iopub.execute_input":"2022-10-09T12:01:18.161370Z","iopub.status.idle":"2022-10-09T12:01:21.987120Z","shell.execute_reply.started":"2022-10-09T12:01:18.161315Z","shell.execute_reply":"2022-10-09T12:01:21.986256Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def loss_fn(x, target):\n    x = x.float()\n    target = target.float()\n    logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n    nll_loss = -logprobs * target\n    nll_loss = nll_loss.sum(-1)\n    smooth_loss = -logprobs.mean(dim=-1)\n    loss = 0.9 * nll_loss + 0.1 * smooth_loss\n    return loss.mean()\n\n\ndef reduce_fn(vals):\n    return sum(vals) / len(vals)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:21.988530Z","iopub.execute_input":"2022-10-09T12:01:21.988891Z","iopub.status.idle":"2022-10-09T12:01:21.996347Z","shell.execute_reply.started":"2022-10-09T12:01:21.988847Z","shell.execute_reply":"2022-10-09T12:01:21.995193Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_loop_fn(trainloader, model, optimizer, device, scheduler=None):\n    model.train()\n    model = model.to(device)\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n    counter = 0\n    for step, (targets, inputs, attention_masks) in enumerate(trainloader):\n        inputs = inputs.to(device, dtype=torch.long)\n        attention_masks = attention_masks.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        output = model(input_ids=inputs, attention_mask=attention_masks)\n        loss = loss_fn(output, targets)\n        correct_predictions += (output.argmax(1) == targets.argmax(1)).type(torch.float).sum().item()\n        total_predictions += len(targets)\n        if step % 50 == 0:\n            loss_reduced = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n            xm.master_print(f\"step: {step} {correct_predictions / total_predictions}, {loss_reduced}\")\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        if scheduler is not None:\n            scheduler.step()\n    xm.master_print(\", accuracy = [{}]\".format(correct_predictions / total_predictions))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:21.997742Z","iopub.execute_input":"2022-10-09T12:01:21.997993Z","iopub.status.idle":"2022-10-09T12:01:23.791267Z","shell.execute_reply.started":"2022-10-09T12:01:21.997941Z","shell.execute_reply":"2022-10-09T12:01:23.790295Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def eval_loop_fn(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n    counter = 0\n    for step, (targets, inputs, attention_masks) in enumerate(data_loader):\n        with torch.no_grad():\n            inputs = inputs.to(device, dtype=torch.long)\n            attention_masks = attention_masks.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(input_ids=inputs, attention_mask=attention_masks)\n            correct_predictions += (outputs.argmax(1) == targets.argmax(1)).type(torch.float).sum().item()\n            total_predictions += len(targets)\n            targets_np = targets.cpu().detach().tolist()\n            outputs_np = outputs.cpu().detach().tolist()\n            fin_targets.extend(targets_np)\n            fin_outputs.extend(outputs_np)    \n            del targets_np, outputs_np\n            gc.collect()\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:23.794350Z","iopub.execute_input":"2022-10-09T12:01:23.794746Z","iopub.status.idle":"2022-10-09T12:01:23.809570Z","shell.execute_reply.started":"2022-10-09T12:01:23.794702Z","shell.execute_reply":"2022-10-09T12:01:23.808464Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data1 = pd.read_csv(\"../input/datalang/test_data.csv\")\ndata1['lang'] = np.where(data1['lang'] == 'kz', 0, 1)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:23.811767Z","iopub.execute_input":"2022-10-09T12:01:23.814154Z","iopub.status.idle":"2022-10-09T12:01:23.892640Z","shell.execute_reply.started":"2022-10-09T12:01:23.814102Z","shell.execute_reply":"2022-10-09T12:01:23.891493Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_dataset = DatasetRetriever(data)\nvalid_dataset = DatasetRetriever(data1)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:23.894629Z","iopub.execute_input":"2022-10-09T12:01:23.895586Z","iopub.status.idle":"2022-10-09T12:01:28.664581Z","shell.execute_reply.started":"2022-10-09T12:01:23.895522Z","shell.execute_reply":"2022-10-09T12:01:28.663691Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3241112f5a70478b946c2f208efae987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6da7d55cd6364c22b3f75e363d667e67"}},"metadata":{}}]},{"cell_type":"code","source":"def _run():\n    MAX_LEN = 192\n    BATCH_SIZE = 16 \n    EPOCHS = 5 \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(), \n          shuffle=True)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0,\n    )\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=0\n    )\n    device = xm.xla_device() \n    model = mx.to(device) \n    xm.master_print('done loading model')\n\n    param_optimizer = list(model.named_parameters()) \n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    xm.master_print('training on train dataset')\n    \n    lr = 0.5e-5 * xm.xrt_world_size() \n    num_train_steps = int(len(train_dataset) / BATCH_SIZE / xm.xrt_world_size() * EPOCHS) \n    \n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr) \n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n    \n    \n    for epoch in range(EPOCHS):\n        gc.collect()\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        xm.master_print('parallel loader created... training now')\n        gc.collect()\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n        del para_loader\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        gc.collect()\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        del para_loader\n        gc.collect()\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        auc_reduced = xm.mesh_reduce('auc_reduce',auc,reduce_fn)\n        xm.master_print(f'AUC = {auc_reduced}')\n        gc.collect()\n    xm.save(model.state_dict(), \"model.bin\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:28.667260Z","iopub.execute_input":"2022-10-09T12:01:28.667609Z","iopub.status.idle":"2022-10-09T12:01:28.684145Z","shell.execute_reply.started":"2022-10-09T12:01:28.667566Z","shell.execute_reply":"2022-10-09T12:01:28.683114Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef _mp_fn(rank, flags):\n    a = _run()\n\nFLAGS={}\nstart_time = time.time()\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","metadata":{"execution":{"iopub.status.busy":"2022-10-09T12:01:28.685602Z","iopub.execute_input":"2022-10-09T12:01:28.685852Z","iopub.status.idle":"2022-10-09T12:14:56.941693Z","shell.execute_reply.started":"2022-10-09T12:01:28.685824Z","shell.execute_reply":"2022-10-09T12:14:56.940526Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"done loading model\ntraining on train dataset\nnum_training_steps = 10035, world_size=8\nparallel loader created... training now\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"step: 0 1.0, 0.4169921875\nstep: 50 0.8676470588235294, 0.33154296875\n, accuracy = [0.8814489571899012]\nAUC = 0.8376165790010087\nparallel loader created... training now\nstep: 0 0.0, 2.6875\nstep: 50 0.6985294117647058, 0.389892578125\n, accuracy = [0.7299670691547749]\nAUC = 0.782102144737878\nparallel loader created... training now\nstep: 0 0.0, 1.5908203125\nstep: 50 0.7365196078431373, 0.322265625\n, accuracy = [0.7639956092206367]\nAUC = 0.9994443072504372\nparallel loader created... training now\nstep: 0 1.0, 0.30517578125\nstep: 50 0.9889705882352942, 0.2303466796875\n, accuracy = [0.9901207464324918]\nAUC = 0.9999871725220331\nparallel loader created... training now\nstep: 0 1.0, 0.2076416015625\nstep: 50 0.9987745098039216, 0.2286376953125\n, accuracy = [0.9978046103183315]\nAUC = 0.999991511227816\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
